{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Invoice Dataset Preprocessing\n",
    "\n",
    "This notebook-like file (cell-based) prepares the dataset for training. It reads CSV annotations and image JPG files, builds input-label pairs, and writes a processed_dataset folder with train/val/test splits.\n"
   ],
   "id": "c66f961ac3398a71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "DATASETS_DIR = PROJECT_ROOT / 'datasets'\n",
    "PROCESSED_DIR = PROJECT_ROOT / 'processed_dataset'\n",
    "PROCESSED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print('Project root:', PROJECT_ROOT)\n",
    "print('Datasets dir:', DATASETS_DIR)\n",
    "print('Processed dir:', PROCESSED_DIR)\n"
   ],
   "id": "48ad537eb9968d52"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Discover batch CSV files in datasets/\n",
   "id": "7d3181a474103e4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "csv_paths = sorted(DATASETS_DIR.glob('*.csv'))\n",
    "print('Found CSVs:', [p.name for p in csv_paths])\n"
   ],
   "id": "f8c10a725784d3cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loader for a single batch CSV -> records\n",
   "id": "4a84803b5a92a647"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_batch(csv_path: Path) -> List[Dict]:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    col_map = {c.lower().strip(): c for c in df.columns}\n",
    "\n",
    "    def get_col(name: str) -> str:\n",
    "        target = name.replace(' ', '').lower()\n",
    "        for key, orig in col_map.items():\n",
    "            if key.replace(' ', '') == target:\n",
    "                return orig\n",
    "        raise KeyError(f'Missing expected column: {name} in {csv_path}')\n",
    "\n",
    "    fname_col = get_col('File Name')\n",
    "    json_col = get_col('Json Data')\n",
    "    ocr_col = get_col('OCRed Text')\n",
    "\n",
    "    images_dir = csv_path.with_suffix('')\n",
    "    if not images_dir.exists():\n",
    "        alt = csv_path.parent / csv_path.stem\n",
    "        if alt.exists():\n",
    "            images_dir = alt\n",
    "    if not images_dir.exists():\n",
    "        print(f'Warning: images dir not found for {csv_path}, tried {images_dir}')\n",
    "\n",
    "    records: List[Dict] = []\n",
    "    for _, row in df.iterrows():\n",
    "        fname = str(row[fname_col]).strip()\n",
    "        img_path = images_dir / fname\n",
    "        json_raw = row[json_col]\n",
    "        ocred_text = row[ocr_col] if pd.notna(row[ocr_col]) else ''\n",
    "\n",
    "        json_data = None\n",
    "        if isinstance(json_raw, str):\n",
    "            try:\n",
    "                json_data = json.loads(json_raw)\n",
    "            except Exception:\n",
    "                json_data = None\n",
    "\n",
    "        rec: Dict = {\n",
    "            'filename': fname,\n",
    "            'image_path': str(img_path),\n",
    "            'ocred_text': str(ocred_text) if not pd.isna(ocred_text) else '',\n",
    "            'json_raw': json_raw if isinstance(json_raw, str) else json.dumps(json_raw)\n",
    "        }\n",
    "        if json_data and isinstance(json_data, dict):\n",
    "            inv = json_data.get('invoice', {})\n",
    "            rec['invoice_number'] = inv.get('invoice_number', '')\n",
    "            rec['invoice_date'] = inv.get('invoice_date', '')\n",
    "            rec['client_name'] = inv.get('client_name', '')\n",
    "            rec['seller_name'] = inv.get('seller_name', '')\n",
    "        else:\n",
    "            rec['invoice_number'] = ''\n",
    "            rec['invoice_date'] = ''\n",
    "            rec['client_name'] = ''\n",
    "            rec['seller_name'] = ''\n",
    "        records.append(rec)\n",
    "    return records\n"
   ],
   "id": "492166c70b141e34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load all batches\n",
   "id": "8428372040f6cccf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "all_records: List[Dict] = []\n",
    "for csv_path in csv_paths:\n",
    "    print('Loading', csv_path.name)\n",
    "    all_records.extend(load_batch(csv_path))\n",
    "\n",
    "print('Total loaded records:', len(all_records))\n"
   ],
   "id": "d4b7036d9fb8834e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Keep only records whose image exists\n",
   "id": "68d1107af325d60f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "valid_records = [r for r in all_records if Path(r['image_path']).exists()]\n",
    "missing = len(all_records) - len(valid_records)\n",
    "print(f'Total records: {len(all_records)}, valid with images: {len(valid_records)}, missing images: {missing}')\n"
   ],
   "id": "19ef6d0b14c53728"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Shuffle and split into train/val/test = 80/10/10\n",
   "id": "ad4dd8440133c1ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "random.seed(42)\n",
    "sz = len(valid_records)\n",
    "if sz == 0:\n",
    "    print('No valid records with existing images found. Nothing to write.')\n",
    "else:\n",
    "    random.shuffle(valid_records)\n",
    "    n_train = int(0.8 * sz)\n",
    "    n_val = int(0.1 * sz)\n",
    "    n_test = sz - n_train - n_val\n",
    "    splits = {\n",
    "        'train': valid_records[:n_train],\n",
    "        'val': valid_records[n_train:n_train+n_val],\n",
    "        'test': valid_records[n_train+n_val:]\n",
    "    }\n",
    "    print({k: len(v) for k, v in splits.items()})\n",
    "\n",
    "    #%% md\n",
    "    ### Write out processed_dataset/<split> with images/ and labels.csv\n",
    "\n",
    "    #%%\n",
    "    def write_split(name: str, recs):\n",
    "        split_dir = PROCESSED_DIR / name\n",
    "        img_out = split_dir / 'images'\n",
    "        split_dir.mkdir(parents=True, exist_ok=True)\n",
    "        img_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        rows = []\n",
    "        for r in recs:\n",
    "            src = Path(r['image_path'])\n",
    "            dst = img_out / src.name\n",
    "            try:\n",
    "                if not dst.exists():\n",
    "                    shutil.copy2(src, dst)\n",
    "            except Exception as e:\n",
    "                print(f'Copy failed for {src} -> {dst}: {e}')\n",
    "                continue\n",
    "            rows.append({\n",
    "                'filename': src.name,\n",
    "                'ocred_text': r.get('ocred_text', ''),\n",
    "                'json_raw': r.get('json_raw', ''),\n",
    "                'invoice_number': r.get('invoice_number',''),\n",
    "                'invoice_date': r.get('invoice_date',''),\n",
    "                'client_name': r.get('client_name',''),\n",
    "                'seller_name': r.get('seller_name','')\n",
    "            })\n",
    "        df = pd.DataFrame(rows)\n",
    "        df.to_csv(split_dir / 'labels.csv', index=False)\n",
    "        print(f'Wrote {len(rows)} records to {split_dir}')\n",
    "\n",
    "    for name, recs in splits.items():\n",
    "        write_split(name, recs)\n",
    "\n",
    "    print('Done.')\n"
   ],
   "id": "652fcc04770317aa"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
